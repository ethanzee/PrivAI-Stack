# ===========================
# vLLM 服务配置
# ===========================

# 1. 镜像版本
# 建议用 latest，或者查阅 DockerHub 找具体存在的版本
# IMAGE_TAG=latest  # <--- 修改了这里
IMAGE_TAG=v0.14.0-cu130  # <--- 修改了这里

# 2. 模型路径 (关键修改)
# 必须指向容器内的绝对路径。
# 前提：你的宿主机 ~/llm_models/modelscope 目录下必须有一个文件夹叫 "Qwen2.5-7B-Instruct"
MODEL_PATH=Qwen/Qwen2.5-7B-Instruct # <--- 修改了这里，使用本地路径

# 3. 对外服务的模型名称
# 这是 Dify 或者 OpenAI 客户端调用时填写的 "model": "xxx"
MODEL_NAME=Qwen3-14B-FP8

# 4. HuggingFace Token
# 用本地模型时，这里留空即可
HF_TOKEN=

# 5. 对外暴露的端口
PORT=7000

# ===========================
# 性能与硬件配置
# ===========================

# 6. 张量并行度
TP_SIZE=1

# 7. 显存利用率
# 如果你的显卡显存比较紧张（比如 16G 跑 7B），可以改小一点到 0.85
GPU_MEMORY_UTILIZATION=0.7

# 8. 最大上下文长度
# 4096 是比较安全的，显存够大(24G+)可以改 8192
MAX_MODEL_LEN=4096

# 9. 数据类型
DTYPE=auto