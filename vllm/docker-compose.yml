services:
  vllm-server:
    image: vllm/vllm-openai:${IMAGE_TAG}
    container_name: vllm-server
    restart: unless-stopped
    # å¯ç”¨ NVIDIA Runtime
    # runtime: nvidia
    
    # å…±äº«å†…å­˜é…ç½®ï¼ŒPyTorch åˆ†å¸ƒå¼æ¨ç†å¿…é¡»å¼€å¯ host æ¨¡å¼
    ipc: host
    
    # ç¯å¢ƒå˜é‡æ˜ å°„
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    
    # ç«¯å£æ˜ å°„
    ports:
      - "${PORT}:8000"
    
    # æŒ‚è½½å·
    volumes:
      - /home/ethan/llm_models/modelscope:/app/model
    

    # ğŸ‘‡ å…³é”®ï¼šåŠ å…¥ Dify çš„ç½‘ç»œï¼Œå¹¶è®¾ç½®åˆ«å
    networks:
      dify_net:
        aliases:
          - vllm  # â† è¿™æ · Dify å°±èƒ½ç”¨ http://vllm:8000 è®¿é—®

    # èµ„æºç®¡ç†ä¸ GPU è°ƒåº¦
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # ä½¿ç”¨å®¿ä¸»æœºæ‰€æœ‰å¯è§ GPU
              capabilities: [gpu]



    # å¯åŠ¨å‘½ä»¤
    command: >
      --model /app/model/${MODEL_PATH}
      --tensor-parallel-size ${TP_SIZE}
      --max-model-len ${MAX_MODEL_LEN}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --dtype ${DTYPE}
      --trust-remote-code
      --served-model-name ${MODEL_NAME}
      # æ³¨æ„ï¼š--served-model-name è®¾ç½®ä¸º gpt-3.5-turbo æ˜¯ä¸ºäº†æ¬ºéª—ä¸€äº›
      # åªè®¤ OpenAI æ¨¡å‹åçš„æ—§å®¢æˆ·ç«¯è½¯ä»¶ï¼Œä½ ä¹Ÿå¯ä»¥æ”¹æˆ ${MODEL_NAME}


# ğŸ‘‡ å®šä¹‰å¤–éƒ¨ç½‘ç»œ
networks:
  dify_net:
    external: true
    name: docker_default  # â† å¿…é¡»å’Œä½ æŸ¥åˆ°çš„ç½‘ç»œåä¸€è‡´ï¼
